---
title: "HW8 - ANSWER KEY"
author: "Vicki Hertzberg, Melinda Higgins"
date: "4/20/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(error = FALSE)
```

K-nearest neighbor

Let's try a variation on the NHANES data set again.

```{r}
library(tidyverse)
library(class)
library(rpart)
library(NHANES)
library(RColorBrewer)
library(plot3D)
library(parallel)
library(randomForestSRC)
library(ggRandomForests)
library(mosaic)

# Create the NHANES dataset again
```

Create the NHANES dataset again, just like we did in class, only using sleep trouble (variable name = `SleepTrouble`) as the dependent variable, instead of `Diabetes`. 

#### Problem 1

What is the marginal distribution of sleep trouble (`SleepTrouble`)?

**ANSWER KEY**

Note: The code for the answer key below was adapted from the class lecture on Unsupervised Learning. See [https://cdn.rawgit.com/vhertzb/2018week11/1debc8ed/Week11.html?raw=true](https://cdn.rawgit.com/vhertzb/2018week11/1debc8ed/Week11.html?raw=true).

```{r}
#Insert your code here to determine the marginal distribution of sleep trouble in the NHANES dataset.

#Code adapted from Unsupervised Learning Lecture
# see https://cdn.rawgit.com/vhertzb/2018week11/1debc8ed/Week11.html?raw=true 

# Create the NHANES dataset again
people <- NHANES %>% dplyr::select(Age, Gender, SleepTrouble, BMI, HHIncome, PhysActive) 
#%>% na.omit()

glimpse(people)

# What is the marginal distribution of Diabetes?
tally(~ SleepTrouble, data = people, format = "percent")

```

Recall from our prior work, the packages work better if the dataset is a dataframe, and the variables are numeric.

**ANSWER KEY**

```{r}

# Insert your code here to make the conversions

# Convert back to dataframe
people <- as.data.frame(people)
glimpse(people)

# Convert factors to numeric - the packages just seem to work better that way
people$Gender <- as.numeric(people$Gender)
people$SleepTrouble <- as.numeric(people$SleepTrouble)
people$HHIncome <- as.numeric(people$HHIncome)
people$PhysActive <- as.numeric(people$PhysActive)

people <- na.omit(people)

glimpse(people)
```

Apply the k-nearest neighbor procedure to predict `SleepTrouble` from the other covariates, as we did for `Diabetes`. Use k = 1, 3, 5, and 20.

#### Problem 2

**ANSWER KEY**

```{r}

# Apply k-nearest neighbor approach to predict 
# SleepTrouble for k = 1, 3, 5, 20
# Insert your code here

# Apply knn procedure to predict Diabetes

# Let's try different values of k to see how that affects performance
knn.1 <- knn(train = people, test = people, 
             cl = as.numeric(people$SleepTrouble), k = 1)
knn.3 <- knn(train = people, test = people, 
             cl = people$SleepTrouble, k = 3)
knn.5 <- knn(train = people, test = people, 
             cl = people$SleepTrouble, k = 5)
knn.20 <- knn(train = people, test = people, 
              cl = people$SleepTrouble, k = 20)

```

Now let's see how well these classifiers work overall

#### Problem 3

**ANSWER KEY**

```{r}

# How well do these classifiers (k = 1, 3, 5, 20) work?
# Insert your code here

# Calculate the percent predicted correctly
100*sum(people$SleepTrouble == knn.1)/length(knn.1)
100*sum(people$SleepTrouble == knn.3)/length(knn.3)
100*sum(people$SleepTrouble == knn.5)/length(knn.5)
100*sum(people$SleepTrouble == knn.20)/length(knn.20)
```

#### Problem 4

What about success overall?

**ANSWER KEY**

```{r}

#Insert your code here to determine overall success for k = 1, 3, 5, 20

# Another way to look at success rate against increasing k
table(knn.1, people$SleepTrouble)
table(knn.3, people$SleepTrouble)
table(knn.5, people$SleepTrouble)
table(knn.20, people$SleepTrouble)
```


