---
title: "HW8"
author: "Vicki Hertzberg"
date: "4/11/2018"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(error = FALSE)
```

K-nearest neighbor

Let's try a variation on the NHANES data set again.

```{r}
library(tidyverse)
library(class)
library(rpart)
library(NHANES)
library(RColorBrewer)
library(plot3D)
library(parallel)
library(randomForestSRC)
library(ggRandomForests)
library(mosaic)

# Create the NHANES dataset again
```

Create the NHANES dataset again, just like we did in class, only using sleep trouble (variable name = `SleepTrouble`) as the dependent variable, instead of `Diabetes`. 

#### Problem 1

What is the marginal distribution of sleep trouble (`SleepTrouble`)?

```{r}

#Insert your code here to determine the marginal distribution of sleep trouble in the NHANES dataset.

```

Recall from our prior work, the packages work better if the dataset is a dataframe, and the variables are numeric.



```{r}

# Insert your code here to make the conversions

```

Apply the k-nearest neighbor procedure to predict `SleepTrouble` from the other covariates, as we did for `Diabetes`. Use k = 1, 3, 5, and 20.

#### Problem 2

```{r}

#Apply k-nearest neighbor approach to predict SleepTrouble for k = 1, 3, 5, 20
#Insert your code here


```

Now let's see how well these classifiers work overall

#### Problem 3

```{r}

# How well do these classifiers (k = 1, 3, 5, 20) work?
# Insert your code here


```

#### Problem 4

What about success overall?

```{r}

#Insert your code here to determine overall success for k = 1, 3, 5, 20

```


